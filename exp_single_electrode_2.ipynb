{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:39:15 gpu 0.0G ram 0.5G] (0) Using device: cuda\n",
      "[11:39:15 gpu 0.0G ram 0.5G] (0) Subject: btbank3, Trial: 1, loading data...\n",
      "[11:40:26 gpu 0.0G ram 10.5G] (0) Data shape: torch.Size([2048])\n",
      "[11:40:26 gpu 0.0G ram 10.5G] (0) Loading the eval trial...\n",
      "[11:41:13 gpu 0.0G ram 17.0G] (0) Done.\n"
     ]
    }
   ],
   "source": [
    "from train_model_single_electrode_new_lin import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "log(f'Using device: {device}')\n",
    "dtype = torch.float32\n",
    "\n",
    "subject_id, trial_id = (3, 1) # changed from 3, 0\n",
    "window_size = 2048\n",
    "subject = BrainTreebankSubject(subject_id, cache=True)\n",
    "\n",
    "log(f'Subject: {subject.subject_identifier}, Trial: {trial_id}, loading data...')\n",
    "electrode_subset = subject.electrode_labels\n",
    "eval_electrode_index = electrode_subset.index('T1cIe11')\n",
    "subject.set_electrode_subset(electrode_subset)\n",
    "dataset = SubjectTrialDataset_SingleElectrode(subject, trial_id, window_size=window_size, dtype=dtype, unsqueeze_electrode_dimension=False, electrodes_subset=electrode_subset)\n",
    "log(\"Data shape: \" + str(dataset[0]['data'].shape))\n",
    "\n",
    "log(\"Loading the eval trial...\")\n",
    "subject.load_neural_data(0)\n",
    "log(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:49:37 gpu 10.3G ram 18.8G] (0) Creating models...\n",
      "[11:49:40 gpu 10.3G ram 18.8G] (0) Training model...\n",
      "[11:49:40 gpu 10.3G ram 18.8G] (0) Evaluating the model before training...\n",
      "[11:51:08 gpu 10.3G ram 18.8G] (0)         {'eval_auroc/btbank3_0_speech': 0.7899697076353438, 'eval_acc/btbank3_0_speech': 0.7166997847325716, 'eval_auroc/average_speech': 0.7899697076353438, 'eval_acc/average_speech': 0.7166997847325716, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6275235639695034, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5971399671479208, 'eval_auroc/average_gpt2_surprisal': 0.6275235639695034, 'eval_acc/average_gpt2_surprisal': 0.5971399671479208}\n",
      "[11:52:07 gpu 15.1G ram 18.8G] (0) Step 100, Loss: 3.4815, LR: 0.003000\n",
      "[11:53:31 gpu 15.1G ram 18.8G] (0)         {'train_loss': 3.4815049171447754, 'step': 100, 'eval_auroc/btbank3_0_speech': 0.7930816221395186, 'eval_acc/btbank3_0_speech': 0.7189699644617678, 'eval_auroc/average_speech': 0.7930816221395186, 'eval_acc/average_speech': 0.7189699644617678, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6284182648130929, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5960255900406328, 'eval_auroc/average_gpt2_surprisal': 0.6284182648130929, 'eval_acc/average_gpt2_surprisal': 0.5960255900406328}\n",
      "[11:53:31 gpu 15.1G ram 18.8G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n",
      "[11:54:30 gpu 15.1G ram 18.8G] (0) Step 200, Loss: 3.3118, LR: 0.003000\n",
      "[11:56:03 gpu 15.1G ram 18.9G] (0)         {'train_loss': 3.311760187149048, 'step': 200, 'eval_auroc/btbank3_0_speech': 0.7941416525121967, 'eval_acc/btbank3_0_speech': 0.7146805380411938, 'eval_auroc/average_speech': 0.7941416525121967, 'eval_acc/average_speech': 0.7146805380411938, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6256952191491785, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5919360248984179, 'eval_auroc/average_gpt2_surprisal': 0.6256952191491785, 'eval_acc/average_gpt2_surprisal': 0.5919360248984179}\n",
      "[11:56:03 gpu 15.1G ram 18.9G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n",
      "[11:57:02 gpu 15.1G ram 18.9G] (0) Step 300, Loss: 3.1889, LR: 0.003000\n",
      "[11:58:53 gpu 15.1G ram 19.0G] (0)         {'train_loss': 3.188905715942383, 'step': 300, 'eval_auroc/btbank3_0_speech': 0.79695112846399, 'eval_acc/btbank3_0_speech': 0.7154374769128868, 'eval_auroc/average_speech': 0.79695112846399, 'eval_acc/average_speech': 0.7154374769128868, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6263388418066772, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5908209561684102, 'eval_auroc/average_gpt2_surprisal': 0.6263388418066772, 'eval_acc/average_gpt2_surprisal': 0.5908209561684102}\n",
      "[11:58:53 gpu 15.1G ram 19.0G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n",
      "[11:59:52 gpu 15.1G ram 19.0G] (0) Step 400, Loss: 3.1761, LR: 0.003000\n",
      "[12:01:38 gpu 15.1G ram 19.1G] (0)         {'train_loss': 3.1760921478271484, 'step': 400, 'eval_auroc/btbank3_0_speech': 0.799494152783978, 'eval_acc/btbank3_0_speech': 0.7217461500248386, 'eval_auroc/average_speech': 0.799494152783978, 'eval_acc/average_speech': 0.7217461500248386, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6232808707817198, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5867312181205152, 'eval_auroc/average_gpt2_surprisal': 0.6232808707817198, 'eval_acc/average_gpt2_surprisal': 0.5867312181205152}\n",
      "[12:01:38 gpu 15.1G ram 19.1G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n",
      "[12:02:38 gpu 15.1G ram 19.2G] (0) Step 500, Loss: 3.2152, LR: 0.003000\n",
      "[12:04:28 gpu 15.1G ram 19.2G] (0)         {'train_loss': 3.2151968479156494, 'step': 500, 'eval_auroc/btbank3_0_speech': 0.7997080825300045, 'eval_acc/btbank3_0_speech': 0.718212388704192, 'eval_auroc/average_speech': 0.7997080825300045, 'eval_acc/average_speech': 0.718212388704192, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6209396845087667, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5848726549667157, 'eval_auroc/average_gpt2_surprisal': 0.6209396845087667, 'eval_acc/average_gpt2_surprisal': 0.5848726549667157}\n",
      "[12:04:28 gpu 15.1G ram 19.2G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n",
      "[12:05:27 gpu 15.1G ram 19.3G] (0) Step 600, Loss: 3.2141, LR: 0.003000\n",
      "[12:07:33 gpu 15.1G ram 19.3G] (0)         {'train_loss': 3.2141470909118652, 'step': 600, 'eval_auroc/btbank3_0_speech': 0.8059483083641545, 'eval_acc/btbank3_0_speech': 0.723257161781752, 'eval_auroc/average_speech': 0.8059483083641545, 'eval_acc/average_speech': 0.723257161781752, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6174086889580461, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5863592980029393, 'eval_auroc/average_gpt2_surprisal': 0.6174086889580461, 'eval_acc/average_gpt2_surprisal': 0.5863592980029393}\n",
      "[12:07:33 gpu 15.1G ram 19.3G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n",
      "[12:08:32 gpu 15.1G ram 19.4G] (0) Step 700, Loss: 3.2167, LR: 0.003000\n",
      "[12:10:39 gpu 15.1G ram 19.4G] (0)         {'train_loss': 3.2167046070098877, 'step': 700, 'eval_auroc/btbank3_0_speech': 0.8116580756808602, 'eval_acc/btbank3_0_speech': 0.7313284165743182, 'eval_auroc/average_speech': 0.8116580756808602, 'eval_acc/average_speech': 0.7313284165743182, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6185180159522976, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5869167459151033, 'eval_auroc/average_gpt2_surprisal': 0.6185180159522976, 'eval_acc/average_gpt2_surprisal': 0.5869167459151033}\n",
      "[12:10:39 gpu 15.1G ram 19.4G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n",
      "[12:11:39 gpu 15.1G ram 19.5G] (0) Step 800, Loss: 3.1850, LR: 0.003000\n",
      "[12:13:46 gpu 15.1G ram 19.5G] (0)         {'train_loss': 3.1850249767303467, 'step': 800, 'eval_auroc/btbank3_0_speech': 0.8161826454762109, 'eval_acc/btbank3_0_speech': 0.7348602672373165, 'eval_auroc/average_speech': 0.8161826454762109, 'eval_acc/average_speech': 0.7348602672373165, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.620564532112421, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5854282009164001, 'eval_auroc/average_gpt2_surprisal': 0.620564532112421, 'eval_acc/average_gpt2_surprisal': 0.5854282009164001}\n",
      "[12:13:46 gpu 15.1G ram 19.5G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n",
      "[12:14:45 gpu 15.1G ram 19.6G] (0) Step 900, Loss: 3.1690, LR: 0.003000\n",
      "[12:16:50 gpu 15.1G ram 19.6G] (0)         {'train_loss': 3.168985366821289, 'step': 900, 'eval_auroc/btbank3_0_speech': 0.8178869419585777, 'eval_acc/btbank3_0_speech': 0.7378889780529124, 'eval_auroc/average_speech': 0.8178869419585777, 'eval_acc/average_speech': 0.7378889780529124, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6219989631710037, 'eval_acc/btbank3_0_gpt2_surprisal': 0.5889613555805309, 'eval_auroc/average_gpt2_surprisal': 0.6219989631710037, 'eval_acc/average_gpt2_surprisal': 0.5889613555805309}\n",
      "[12:16:50 gpu 15.1G ram 19.6G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n",
      "[12:17:50 gpu 15.1G ram 19.7G] (0) Step 1000, Loss: 3.2203, LR: 0.003000\n",
      "[12:19:57 gpu 15.1G ram 19.7G] (0)         {'train_loss': 3.220339298248291, 'step': 1000, 'eval_auroc/btbank3_0_speech': 0.8215768477847709, 'eval_acc/btbank3_0_speech': 0.7396550626058823, 'eval_auroc/average_speech': 0.8215768477847709, 'eval_acc/average_speech': 0.7396550626058823, 'eval_auroc/btbank3_0_gpt2_surprisal': 0.6212702383986037, 'eval_acc/btbank3_0_gpt2_surprisal': 0.589889513270511, 'eval_auroc/average_gpt2_surprisal': 0.6212702383986037, 'eval_acc/average_gpt2_surprisal': 0.589889513270511}\n",
      "[12:19:57 gpu 15.1G ram 19.7G] (0) Saved training logs to eval_results/juno/btbank3_1_embed128_resolution10.json\n"
     ]
    }
   ],
   "source": [
    "d_embed = 128\n",
    "resolution = 10\n",
    "n_steps = 1000\n",
    "batch_size = 128\n",
    "log_every_step = min(100, n_steps//10)\n",
    "\n",
    "save_dir = \"eval_results/juno/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "filename = f'{subject.subject_identifier}_{trial_id}_embed{d_embed}_resolution{resolution}.json'\n",
    "\n",
    "log(f'Creating models...')\n",
    "import itertools\n",
    "dataloader = iter(torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True))\n",
    "dataloader = iter(itertools.cycle(dataloader))\n",
    "\n",
    "embed = EmbedderDiscretized(d_model=d_embed, resolution=resolution, range=(-3, 3))\n",
    "unembed = EmbedderDiscretized(d_model=d_embed, resolution=resolution, range=(-3, 3))\n",
    "\n",
    "model = ContrastiveModel(d_input=n_samples_per_bin, embed=embed, unembed=unembed).to(device, dtype=dtype)\n",
    "masker = NoneMasker()\n",
    "\n",
    "# Create samples from 10 random indices of the dataset\n",
    "samples = torch.cat([dataset[random.randint(0, len(dataset)-1)]['data'].flatten() for _ in range(n_samples_inverter)])\n",
    "inverter = DistributionInverter(samples=samples).to(device, dtype=dtype)\n",
    "\n",
    "evaluation = ModelEvaluation_BTBench(model, inverter, [(subject, 0)], [\"speech\", \"gpt2_surprisal\"], feature_aggregation_method='concat', \n",
    "                                        mean_collapse_factor=mean_collapse_factor, eval_electrode_index=eval_electrode_index)\n",
    "\n",
    "log(f'Training model...')\n",
    "initial_lr = 0.003\n",
    "use_muon = True\n",
    "optimizers = []\n",
    "schedulers = []\n",
    "if use_muon:\n",
    "    from muon import Muon\n",
    "    all_params = list(model.parameters())\n",
    "    matrix_params = [p for p in all_params if p.ndim >= 2]\n",
    "    other_params = [p for p in all_params if p.ndim < 2]\n",
    "    optimizers.append(Muon(matrix_params, lr=initial_lr, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5))\n",
    "    if len(other_params) > 0:\n",
    "        optimizers.append(torch.optim.AdamW(other_params, lr=initial_lr, betas=(0.9, 0.95)))\n",
    "    #schedulers.append(None)  # Muon doesn't support schedulers\n",
    "    #schedulers.append(torch.optim.lr_scheduler.LinearLR(optimizers[1], start_factor=1.0, end_factor=0.0, total_iters=n_steps))\n",
    "else:\n",
    "    optimizers = [torch.optim.AdamW(model.parameters(), lr=initial_lr, betas=(0.9, 0.95))]\n",
    "    #schedulers = [torch.optim.lr_scheduler.LinearLR(optimizers[0], start_factor=1.0, end_factor=0.0, total_iters=n_steps)]\n",
    "\n",
    "log(\"Evaluating the model before training...\")\n",
    "evaluation_results = evaluation.evaluate()\n",
    "log(evaluation_results, indent=2)\n",
    "evaluation_results['step'] = 0\n",
    "evaluation_results['train_loss'] = -1\n",
    "training_logs = [evaluation_results]\n",
    "\n",
    "step = 1\n",
    "for batch in dataloader:\n",
    "    for optimizer in optimizers:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    batch_data = batch['data'].to(device, dtype=dtype).reshape(batch_size, window_size//n_samples_per_bin, n_samples_per_bin) # shape (batch_size, seq_len, 1)\n",
    "    batch_data = inverter(batch_data)\n",
    "    masked_x, mask = masker.forward(batch_data)\n",
    "\n",
    "    loss = model.calculate_loss(masked_x.unsqueeze(-2), mask=mask)\n",
    "    loss.backward()\n",
    "    for optimizer in optimizers:\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the schedulers\n",
    "    for scheduler in schedulers:\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # Log metrics\n",
    "    log_dict = {\n",
    "        'train_loss': loss.item(),\n",
    "        'step': step,\n",
    "    }\n",
    "    \n",
    "    if step % log_every_step == 0:\n",
    "        current_lr = optimizers[-1].param_groups[0]['lr']\n",
    "        log(f\"Step {step}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Add evaluation results\n",
    "        evaluation_results = evaluation.evaluate()\n",
    "        log_dict.update(evaluation_results)\n",
    "        log(log_dict, indent=2)\n",
    "        \n",
    "    training_logs.append(log_dict)\n",
    "    \n",
    "    # Save training results to file\n",
    "    if step % log_every_step == 0 or step == n_steps:\n",
    "        # Convert training logs to a format that can be saved as JSON\n",
    "        import json\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(training_logs, f, indent=2)\n",
    "        log(f\"Saved training logs to {save_path}\")\n",
    "        \n",
    "    if step == n_steps:\n",
    "        break # Only process one batch per step\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BFM IC2 (.venv)",
   "language": "python",
   "name": "bfm_ic2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
