{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:57:14 gpu 0.0G ram 0.8G] Directory name: andrii0_lr0.003_wd0.0_dr0.1_rTEMP_t20250812_172908\n",
      "[17:57:14 gpu 0.0G ram 0.8G] Using device: cuda\n",
      "[17:57:14 gpu 0.0G ram 0.8G] Loading subjects...\n",
      "[17:57:14 gpu 0.0G ram 0.8G]     loading subject btbank10...\n",
      "[17:57:14 gpu 0.0G ram 0.8G] Loading model...\n",
      "[17:57:29 gpu 0.0G ram 1.0G] Adding subject btbank10 to electrode embeddings...\n",
      "[17:57:29 gpu 0.0G ram 1.0G] Loading model weights...\n",
      "[17:57:29 gpu 0.1G ram 1.0G] Computing frozen features...\n",
      "[17:57:29 gpu 0.1G ram 1.0G] Generating Neuroprobe Eval indices for all subjects and trials\n",
      "[17:57:30 gpu 0.1G ram 1.0G]     Generated 3500 indices for btbank10_0\n",
      "[17:57:42 gpu 0.1G ram 1.1G]     Generating features for batch 0 of 3500\n",
      "[17:57:57 gpu 2.9G ram 6.5G]     Generating features for batch 100 of 3500\n",
      "[17:58:09 gpu 3.1G ram 6.4G]     Generating features for batch 200 of 3500\n",
      "[17:58:20 gpu 3.1G ram 6.4G]     Generating features for batch 300 of 3500\n",
      "[17:58:31 gpu 3.1G ram 6.4G]     Generating features for batch 400 of 3500\n",
      "[17:58:42 gpu 3.1G ram 6.4G]     Generating features for batch 500 of 3500\n",
      "[17:58:53 gpu 3.1G ram 6.4G]     Generating features for batch 600 of 3500\n",
      "[17:59:04 gpu 3.1G ram 6.4G]     Generating features for batch 700 of 3500\n",
      "[17:59:15 gpu 3.1G ram 6.4G]     Generating features for batch 800 of 3500\n",
      "[17:59:26 gpu 3.1G ram 6.4G]     Generating features for batch 900 of 3500\n",
      "[17:59:38 gpu 3.1G ram 6.4G]     Generating features for batch 1000 of 3500\n",
      "[17:59:49 gpu 3.1G ram 6.4G]     Generating features for batch 1100 of 3500\n",
      "[18:00:00 gpu 3.1G ram 6.4G]     Generating features for batch 1200 of 3500\n",
      "[18:00:11 gpu 3.1G ram 6.4G]     Generating features for batch 1300 of 3500\n",
      "[18:00:22 gpu 3.1G ram 6.4G]     Generating features for batch 1400 of 3500\n",
      "[18:00:33 gpu 3.1G ram 6.4G]     Generating features for batch 1500 of 3500\n",
      "[18:00:44 gpu 3.1G ram 6.4G]     Generating features for batch 1600 of 3500\n",
      "[18:00:55 gpu 3.1G ram 6.4G]     Generating features for batch 1700 of 3500\n",
      "[18:01:06 gpu 3.1G ram 6.4G]     Generating features for batch 1800 of 3500\n",
      "[18:01:17 gpu 3.1G ram 6.4G]     Generating features for batch 1900 of 3500\n",
      "[18:01:28 gpu 3.1G ram 6.4G]     Generating features for batch 2000 of 3500\n",
      "[18:01:40 gpu 3.1G ram 6.4G]     Generating features for batch 2100 of 3500\n",
      "[18:01:51 gpu 3.1G ram 6.4G]     Generating features for batch 2200 of 3500\n",
      "[18:02:02 gpu 3.1G ram 6.4G]     Generating features for batch 2300 of 3500\n",
      "[18:02:13 gpu 3.1G ram 6.4G]     Generating features for batch 2400 of 3500\n",
      "[18:02:24 gpu 3.1G ram 6.4G]     Generating features for batch 2500 of 3500\n",
      "[18:02:35 gpu 3.1G ram 6.4G]     Generating features for batch 2600 of 3500\n",
      "[18:02:46 gpu 3.1G ram 6.4G]     Generating features for batch 2700 of 3500\n",
      "[18:02:57 gpu 3.1G ram 6.4G]     Generating features for batch 2800 of 3500\n",
      "[18:03:09 gpu 3.1G ram 6.4G]     Generating features for batch 2900 of 3500\n",
      "[18:03:20 gpu 3.1G ram 6.4G]     Generating features for batch 3000 of 3500\n",
      "[18:03:31 gpu 3.1G ram 6.4G]     Generating features for batch 3100 of 3500\n",
      "[18:03:42 gpu 3.1G ram 6.4G]     Generating features for batch 3200 of 3500\n",
      "[18:03:53 gpu 3.1G ram 6.4G]     Generating features for batch 3300 of 3500\n",
      "[18:04:04 gpu 3.1G ram 6.4G]     Generating features for batch 3400 of 3500\n",
      "[18:04:14 gpu 3.1G ram 6.4G] Saved results to runs/data/andrii0_lr0.003_wd0.0_dr0.1_rTEMP_t20250812_172908/frozen_features_neuroprobe/model_epoch0/frozen_population_btbank10_0.pth\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from torch.amp import autocast\n",
    "\n",
    "from evaluation.neuroprobe import config as neuroprobe_config\n",
    "from evaluation.neuroprobe.datasets import BrainTreebankSubjectTrialBenchmarkDataset\n",
    "from evaluation.neuroprobe_tasks import FrozenModelEvaluation_SS_SM\n",
    "from subject.dataset import load_subjects\n",
    "from torch.optim.lr_scheduler import ChainedScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from training_setup.training_config import (\n",
    "    convert_dtypes,\n",
    "    get_default_config,\n",
    "    log,\n",
    "    parse_config_from_args,\n",
    "    parse_subject_trials_from_config,\n",
    "    unconvert_dtypes,\n",
    "    update_dir_name,\n",
    "    update_random_seed,\n",
    ")\n",
    "from utils.muon_optimizer import Muon\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "RUNS_DIR='runs/data'\n",
    "\n",
    "### PARSE MODEL DIR ###\n",
    "\n",
    "# Default values instead of argparse\n",
    "model_dir = \"andrii0_lr0.003_wd0.0_dr0.1_rTEMP_t20250812_172908\"  # Directory containing the saved model\n",
    "model_epoch = 0  # Epoch of the model to load\n",
    "subject_id = 10  # Subject identifier \n",
    "trial_id = 0  # Trial identifier\n",
    "eval_tasks = [\"speech\"]  # Tasks to evaluate on\n",
    "overwrite = False  # Whether to overwrite existing frozen features\n",
    "batch_size = 100  # Batch size for feature computation\n",
    "\n",
    "bins_start_before_word_onset_seconds = 0\n",
    "bins_end_after_word_onset_seconds = 1.0\n",
    "\n",
    "### LOAD CONFIG ###\n",
    "\n",
    "# Load the checkpoint\n",
    "if model_epoch < 0: model_epoch = \"final\"\n",
    "checkpoint_path = os.path.join(RUNS_DIR, model_dir, f\"model_epoch_{model_epoch}.pth\")\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "config = unconvert_dtypes(checkpoint['config'])\n",
    "log(f\"Directory name: {model_dir}\", priority=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config['device'] = device\n",
    "log(f\"Using device: {device}\", priority=0)\n",
    "\n",
    "config['training']['train_subject_trials'] = \"\"\n",
    "config['training']['eval_subject_trials'] = f\"btbank{subject_id}_{trial_id}\"\n",
    "parse_subject_trials_from_config(config)\n",
    "\n",
    "if 'setup_name' not in config['training']:\n",
    "    config['training']['setup_name'] = \"andrii0\" # XXX: this is only here for backwards compatibility, can remove soon\n",
    "\n",
    "### LOAD SUBJECTS ###\n",
    "\n",
    "log(f\"Loading subjects...\", priority=0)\n",
    "# all_subjects is a dictionary of subjects, with the subject identifier as the key and the subject object as the value\n",
    "all_subjects = load_subjects(config['training']['train_subject_trials'], \n",
    "                             config['training']['eval_subject_trials'], config['training']['data_dtype'], \n",
    "                             cache=config['cluster']['cache_subjects'], allow_corrupted=False)\n",
    "subject = all_subjects[f\"btbank{subject_id}\"] # we only really have one subject, so we can just get it by subject identifier\n",
    "\n",
    "electrode_subset = neuroprobe_config.NEUROPROBE_LITE_ELECTRODES[f\"btbank{subject_id}\"]\n",
    "\n",
    "### LOAD MODEL ###\n",
    "\n",
    "# Import the training setup class dynamically based on config\n",
    "try:\n",
    "    setup_module = __import__(f'training_setup.{config[\"training\"][\"setup_name\"].lower()}', fromlist=[config[\"training\"][\"setup_name\"]])\n",
    "    setup_class = getattr(setup_module, config[\"training\"][\"setup_name\"])\n",
    "    training_setup = setup_class(all_subjects, config, verbose=True)\n",
    "except (ImportError, AttributeError) as e:\n",
    "    print(f\"Could not load training setup '{config['training']['setup_name']}'. Are you sure the filename and the class name are the same and correspond to the parameter? Error: {str(e)}\")\n",
    "    exit()\n",
    "\n",
    "log(f\"Loading model...\", priority=0)\n",
    "training_setup.initialize_model()\n",
    "\n",
    "log(f\"Loading model weights...\", priority=0)\n",
    "training_setup.load_model(model_epoch)\n",
    "\n",
    "log(f\"Computing frozen features...\", priority=0)\n",
    "from evaluation.neuroprobe_frozen import NeuroprobeFrozenFeaturesExtractor\n",
    "feature_extractor = NeuroprobeFrozenFeaturesExtractor(\n",
    "    training_setup=training_setup,\n",
    "    all_subjects=all_subjects,\n",
    "    device='cpu',\n",
    "    eval_names=eval_tasks,\n",
    "    subject_trials=[(subject_id, trial_id)],\n",
    "    dtype=torch.float32,\n",
    "    feature_aggregation_method=None,\n",
    "    log_priority=0,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "save_file_path = os.path.join(RUNS_DIR, model_dir, \"frozen_features_neuroprobe\", f\"model_epoch{model_epoch}\", f\"frozen_population_btbank{subject_id}_{trial_id}.pth\")\n",
    "os.makedirs(os.path.dirname(save_file_path), exist_ok=True)\n",
    "feature_extractor.generate_frozen_features(save_path=save_file_path)\n",
    "log(f\"Saved results to {save_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BFM Local (.venv)",
   "language": "python",
   "name": "bfm_local"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
