{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:06:03 gpu 0.0G ram 0.5G] (0) Using device: cuda\n",
      "[17:06:03 gpu 0.0G ram 0.5G] (0) Loading the train subjects...\n",
      "[17:06:03 gpu 0.0G ram 0.5G] (0)     Subject: btbank3, Trial: 1, loading data...\n",
      "[17:07:26 gpu 0.0G ram 10.6G] (0) Done.\n",
      "[17:07:26 gpu 0.0G ram 10.6G] (0) Loading the train datasets...\n",
      "[17:07:26 gpu 0.0G ram 10.6G] (0)     Loading subject 3, trial 1...\n",
      "[17:07:26 gpu 0.0G ram 10.6G] (0) Done.\n",
      "[17:07:26 gpu 0.0G ram 10.6G] (0) Data shape: torch.Size([2048]); Length: 1311052\n"
     ]
    }
   ],
   "source": [
    "from train_model_single_electrode_new_lin import *\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import btbench.btbench_config as btbench_config\n",
    "from btbench.braintreebank_subject import BrainTreebankSubject as BTBench_BrainTreebankSubject\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "log(f'Using device: {device}')\n",
    "dtype = torch.float32\n",
    "\n",
    "train_subject_trials = [st for st in btbench_config.BTBENCH_FULL_SUBJECT_TRIALS if st not in btbench_config.BTBENCH_LITE_SUBJECT_TRIALS]\n",
    "train_subject_trials = [(3, 1)]\n",
    "window_size = 2048\n",
    "\n",
    "all_subjects = {}\n",
    "log(\"Loading the train subjects...\")\n",
    "for subject_id, trial_id in train_subject_trials:\n",
    "    if subject_id not in all_subjects:\n",
    "        all_subjects[subject_id] = BrainTreebankSubject(subject_id, cache=True)\n",
    "    subject = all_subjects[subject_id]\n",
    "    log(f'Subject: {subject.subject_identifier}, Trial: {trial_id}, loading data...', indent=1)\n",
    "    subject.load_neural_data(trial_id)\n",
    "log(\"Done.\")\n",
    "\n",
    "datasets = []\n",
    "log(\"Loading the train datasets...\")\n",
    "for subject_id, trial_id in train_subject_trials:\n",
    "    subject = all_subjects[subject_id]\n",
    "    log(f\"Loading subject {subject_id}, trial {trial_id}...\", indent=1)\n",
    "    dataset = SubjectTrialDataset_SingleElectrode(subject, trial_id, window_size=window_size, dtype=dtype, unsqueeze_electrode_dimension=False)\n",
    "    datasets.append(dataset)\n",
    "dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "log(\"Done.\")\n",
    "\n",
    "log(\"Data shape: \" + str(dataset[0]['data'].shape) + \"; Length: \" + str(len(dataset)))\n",
    "\n",
    "eval_subject_id, eval_trial_id = 3, 0\n",
    "eval_subject = BTBench_BrainTreebankSubject(eval_subject_id, cache=True)\n",
    "eval_electrode_index = eval_subject.electrode_labels.index('T1cIe11')\n",
    "\n",
    "batch_size = 256 # up from 128\n",
    "dataloader = iter(torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True))\n",
    "dataloader = iter(itertools.cycle(dataloader))\n",
    "\n",
    "\n",
    "def save_checkpoint(step, training_logs, model, embed, unembed, inverter, save_dir, filename_base):\n",
    "    \"\"\"Save both training logs and model checkpoint in a single function\"\"\"\n",
    "    # Save training logs\n",
    "    log_filename = f'{filename_base}.json'\n",
    "    log_save_path = os.path.join(save_dir, log_filename)\n",
    "    with open(log_save_path, 'w') as f:\n",
    "        json.dump(training_logs, f, indent=2)\n",
    "    log(f\"Saved training logs to {log_save_path}\")\n",
    "    \n",
    "    # Save model and state dictionaries\n",
    "    model_filename = f'{filename_base}_model.pt'\n",
    "    model_save_path = os.path.join(save_dir, model_filename)\n",
    "    save_dict = {\n",
    "        'model': model.state_dict(),\n",
    "        'embed': embed.state_dict(),\n",
    "        'unembed': unembed.state_dict(),\n",
    "        'inverter': inverter.state_dict(),\n",
    "        #'optimizer_states': [optimizer.state_dict() for optimizer in optimizers],\n",
    "        'training_config': {\n",
    "            'subject_id': subject.subject_id,\n",
    "            'trial_id': trial_id,\n",
    "            'window_size': window_size,\n",
    "            'd_embed': d_embed,\n",
    "            'n_steps': n_steps,\n",
    "            'batch_size': batch_size,\n",
    "            'eval_electrode_indices': [eval_electrode_index],\n",
    "            'n_samples_per_bin': n_samples_per_bin\n",
    "        }\n",
    "    }\n",
    "    torch.save(save_dict, model_save_path)\n",
    "    log(f\"Saved model and state dictionaries to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embed = 192\n",
    "n_steps = 3000\n",
    "batch_size = 256 # up from 128\n",
    "log_every_step = min(100, n_steps//10)\n",
    "save_every_step = min(1000, n_steps//10)\n",
    "eval_every_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:07:26 gpu 0.0G ram 10.6G] (0) Creating models...\n"
     ]
    }
   ],
   "source": [
    "n_samples_per_bin = 8\n",
    "\n",
    "n_samples_inverter = 100\n",
    "mean_collapse_factor = 1#//n_samples_per_bin\n",
    "\n",
    "save_dir = 'eval_results/juno6/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "filename_base = f'{subject.subject_identifier}_{trial_id}_embed{d_embed}_nspb{n_samples_per_bin}'\n",
    "\n",
    "log(f'Creating models...')\n",
    "\n",
    "first_kernel = 16\n",
    "second_kernel = 16\n",
    "\n",
    "# Define a CNN model that produces a feature vector every 128 timesteps\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_features=d_embed, first_kernel=16, second_kernel=8):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=first_kernel, stride=first_kernel, padding=0),\n",
    "            # nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Second convolutional layer\n",
    "            nn.Conv1d(32, 64, kernel_size=second_kernel, stride=second_kernel, padding=0),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Third convolutional layer\n",
    "            # nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=0),\n",
    "            # # nn.BatchNorm1d(128),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Final projection to output feature dimension\n",
    "        self.projection = nn.Conv1d(64, output_features, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, n_channels, 1]\n",
    "        # Reshape to [batch_size, 1, sequence_length] for 1D convolution\n",
    "        x = x.unsqueeze(1).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        # Apply convolutional layers\n",
    "        features = self.conv_layers(x)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        features = self.projection(features)\n",
    "        \n",
    "        # Transpose to get [batch_size, out_sequence_length, 1, output_features]\n",
    "        features = features.transpose(1, 2).unsqueeze(-2)\n",
    "        \n",
    "        return features\n",
    "\n",
    "class FFTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, max_frequency_bin=64, samples_per_bin=256, power=True, d_embed=192):\n",
    "        super(FFTFeatureExtractor, self).__init__()\n",
    "        self.max_frequency_bin = max_frequency_bin\n",
    "        self.samples_per_bin = samples_per_bin\n",
    "        self.power = power\n",
    "        self.d_embed = d_embed\n",
    "\n",
    "        self.linear_embed = nn.Linear(max_frequency_bin if power else 2*max_frequency_bin, self.d_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, n_channels=1, samples_per_bin]\n",
    "        batch_size, sequence_length, _, _ = x.shape\n",
    "\n",
    "        # Calculate FFT for each timebin\n",
    "        x = x.reshape(-1, self.samples_per_bin)\n",
    "        x = x.to(dtype=torch.float32)  # Convert to float32 for FFT\n",
    "        x = torch.fft.rfft(x, dim=-1)  # Using rfft for real-valued input\n",
    "\n",
    "        x = x.reshape(batch_size, sequence_length, -1)  # shape: (batch_size, sequence_length, max_frequency_bin)\n",
    "\n",
    "        # Pad or trim to max_frequency_bin dimension\n",
    "        if x.shape[2] < self.max_frequency_bin:\n",
    "            x = torch.nn.functional.pad(x, (0, 0, 0, self.max_frequency_bin - x.shape[2]))\n",
    "        else:\n",
    "            x = x[:, :, :self.max_frequency_bin]\n",
    "\n",
    "        if self.power:\n",
    "            # Calculate magnitude (equivalent to scipy.signal.stft's magnitude)\n",
    "            x = torch.abs(x)\n",
    "            # Convert to power\n",
    "            x = torch.log(x + 1e-5)\n",
    "        else:\n",
    "            x = torch.cat([torch.abs(x), torch.angle(x)], dim=-1) # shape: (batch_size, sequence_length, 2*max_frequency_bin)\n",
    "\n",
    "        x = self.linear_embed(x) # shape: (batch_size, sequence_length, d_model)\n",
    "        x = x.unsqueeze(-2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Test the CNN model with a batch\n",
    "# embed = CNNFeatureExtractor(first_kernel=first_kernel, second_kernel=second_kernel).to(device, dtype=dtype)\n",
    "# embed = FFTFeatureExtractor(max_frequency_bin=64, samples_per_bin=n_samples_per_bin, power=True, d_embed=d_embed).to(device, dtype=dtype)\n",
    "# unembed = embed\n",
    "\n",
    "embed = EmbedderLinear(d_model=d_embed, d_input=n_samples_per_bin)\n",
    "unembed = EmbedderLinear(d_model=d_embed, d_input=n_samples_per_bin)\n",
    "\n",
    "model = ContrastiveModel(d_input=n_samples_per_bin, embed=embed, unembed=unembed,\n",
    "                         d_model=d_embed, n_layers=3, n_heads=6, window_size=window_size).to(device, dtype=dtype)\n",
    "masker = NoneMasker()\n",
    "\n",
    "# Create samples from 10 random indices of the dataset\n",
    "samples = torch.cat([dataset[random.randint(0, len(dataset)-1)]['data'].flatten() for _ in range(n_samples_inverter)])\n",
    "inverter = DistributionInverter(samples=samples).to(device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:08:14 gpu 0.0G ram 17.3G] (0) Training model...\n",
      "[17:08:22 gpu 3.0G ram 18.0G] (0) Step 100, Loss: 5.1962, LR: 0.003000\n",
      "[17:08:29 gpu 3.0G ram 18.2G] (0) Step 200, Loss: 5.2000, LR: 0.003000\n",
      "[17:08:35 gpu 3.0G ram 18.4G] (0) Step 300, Loss: 5.1278, LR: 0.003000\n",
      "[17:08:35 gpu 3.0G ram 18.4G] (0) Saved training logs to eval_results/juno6/btbank3_1_embed192_nspb8.json\n",
      "[17:08:35 gpu 3.0G ram 18.4G] (0) Saved model and state dictionaries to eval_results/juno6/btbank3_1_embed192_nspb8_model.pt\n",
      "[17:08:42 gpu 3.0G ram 18.6G] (0) Step 400, Loss: 5.0743, LR: 0.003000\n",
      "[17:08:48 gpu 3.0G ram 18.8G] (0) Step 500, Loss: 5.0222, LR: 0.003000\n",
      "[17:08:55 gpu 3.0G ram 19.0G] (0) Step 600, Loss: 5.0422, LR: 0.003000\n",
      "[17:08:55 gpu 3.0G ram 19.0G] (0) Saved training logs to eval_results/juno6/btbank3_1_embed192_nspb8.json\n",
      "[17:08:55 gpu 3.0G ram 19.0G] (0) Saved model and state dictionaries to eval_results/juno6/btbank3_1_embed192_nspb8_model.pt\n",
      "[17:09:01 gpu 3.0G ram 19.2G] (0) Step 700, Loss: 5.1230, LR: 0.003000\n",
      "[17:09:08 gpu 3.0G ram 19.4G] (0) Step 800, Loss: 5.3558, LR: 0.003000\n",
      "[17:09:15 gpu 3.0G ram 19.5G] (0) Step 900, Loss: 4.9998, LR: 0.003000\n",
      "[17:09:15 gpu 3.0G ram 19.5G] (0) Saved training logs to eval_results/juno6/btbank3_1_embed192_nspb8.json\n",
      "[17:09:15 gpu 3.0G ram 19.5G] (0) Saved model and state dictionaries to eval_results/juno6/btbank3_1_embed192_nspb8_model.pt\n",
      "[17:09:21 gpu 3.0G ram 19.7G] (0) Step 1000, Loss: 5.1365, LR: 0.003000\n"
     ]
    }
   ],
   "source": [
    "eval_tasks = [\"gpt2_surprisal\", \"speech\"]\n",
    "#eval_tasks = [\"speech\"]\n",
    "evaluation = ModelEvaluation_BTBench(model, inverter, [(eval_subject, eval_trial_id)], eval_tasks, feature_aggregation_method='concat', \n",
    "                                        mean_collapse_factor=mean_collapse_factor, eval_electrode_indices=[eval_electrode_index], n_samples_per_bin=n_samples_per_bin,\n",
    "                                        lite=True, standardize=False)\n",
    "\n",
    "log(f'Training model...')\n",
    "initial_lr = 0.003\n",
    "use_muon = False\n",
    "optimizers = []\n",
    "schedulers = []\n",
    "if use_muon:\n",
    "    from muon import Muon\n",
    "    all_params = list(model.parameters())\n",
    "    matrix_params = [p for p in all_params if p.ndim >= 2]\n",
    "    other_params = [p for p in all_params if p.ndim < 2]\n",
    "    optimizers.append(Muon(matrix_params, lr=initial_lr, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5))\n",
    "    if len(other_params) > 0:\n",
    "        optimizers.append(torch.optim.AdamW(other_params, lr=initial_lr, betas=(0.9, 0.95)))\n",
    "    #schedulers.append(None)  # Muon doesn't support schedulers\n",
    "    #schedulers.append(torch.optim.lr_scheduler.LinearLR(optimizers[1], start_factor=1.0, end_factor=0.0, total_iters=n_steps))\n",
    "else:\n",
    "    optimizers = [torch.optim.AdamW(model.parameters(), lr=initial_lr, betas=(0.9, 0.95))]\n",
    "    #schedulers = [torch.optim.lr_scheduler.LinearLR(optimizers[0], start_factor=1.0, end_factor=0.0, total_iters=n_steps)]\n",
    "\n",
    "# log(\"Evaluating the model before training...\")\n",
    "# evaluation_results = evaluation.evaluate(only_keys_containing='auroc/average')\n",
    "# log(evaluation_results, indent=2)\n",
    "# evaluation_results['step'] = 0\n",
    "# evaluation_results['train_loss'] = -1\n",
    "# training_logs = [evaluation_results]\n",
    "training_logs = []\n",
    "\n",
    "step = 1\n",
    "for batch in dataloader:\n",
    "    for optimizer in optimizers:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    batch_data = batch['data'].to(device, dtype=dtype).reshape(batch_size, window_size//n_samples_per_bin, n_samples_per_bin) # shape (batch_size, seq_len, 1)\n",
    "    batch_data = inverter(batch_data)\n",
    "    masked_x, mask = masker.forward(batch_data)\n",
    "\n",
    "    loss = model.calculate_loss(masked_x.unsqueeze(-2), mask=mask)\n",
    "    loss.backward()\n",
    "    for optimizer in optimizers:\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the schedulers\n",
    "    for scheduler in schedulers:\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # Log metrics\n",
    "    log_dict = {\n",
    "        'train_loss': loss.item(),\n",
    "        'step': step,\n",
    "    }\n",
    "    \n",
    "    if step % log_every_step == 0:\n",
    "        current_lr = optimizers[-1].param_groups[0]['lr']\n",
    "        log(f\"Step {step}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "    if step % eval_every_step == 0:\n",
    "        # Add evaluation results\n",
    "        evaluation_results = evaluation.evaluate(only_keys_containing='auroc/average')\n",
    "        log_dict.update(evaluation_results)\n",
    "        log(log_dict, indent=2)\n",
    "\n",
    "    # Save training results to file\n",
    "    if step % save_every_step == 0 or step == n_steps:\n",
    "        save_checkpoint(\n",
    "            step=step,\n",
    "            training_logs=training_logs,\n",
    "            model=model,\n",
    "            embed=embed,\n",
    "            unembed=unembed,\n",
    "            inverter=inverter,\n",
    "            save_dir=save_dir,\n",
    "            filename_base=filename_base,\n",
    "        )\n",
    "        \n",
    "    training_logs.append(log_dict)\n",
    "        \n",
    "    if step == n_steps:\n",
    "        break # Only process one batch per step\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BFM IC2 (.venv)",
   "language": "python",
   "name": "bfm_ic2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
